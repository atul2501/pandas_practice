{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "846e01f8-7a8b-4b43-bad1-f066f038efe6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The webpage \"India Real Estate Property Site - Buy Sell Rent Properties Portal - 99acres.com\" did get fully laoded.\n",
      "\n",
      "The webpage \"India Real Estate Property Site - Buy Sell Rent Properties Portal - 99acres.com\" did get fully laoded.\n",
      "\n",
      "Timeout because we have uncovered all filters.\n",
      "\n",
      "Timeout while clicking on \"Next Page\".\n",
      "\n",
      "We have scraped 61 pages.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "class ProprertyScraper:\n",
    "\tdef __init__(self, url, timeout=5):\n",
    "\t\tself.url = url\n",
    "\t\tself.data = []\n",
    "\t\tself.driver = self._initialize_driver()\n",
    "\t\tself.wait = WebDriverWait(self.driver, timeout=timeout)\n",
    "\n",
    "\n",
    "\tdef _initialize_driver(self):\n",
    "\t\tchrome_options = Options()\n",
    "\t\tchrome_options.add_argument(\"--disable-http2\")\n",
    "\t\tchrome_options.add_argument(\"--incognito\")\n",
    "\t\tchrome_options.add_argument(\"--disable-blink-features=AutomationControlled\")\n",
    "\t\tchrome_options.add_argument(\"--ignore-certificate-errors\")\n",
    "\t\tchrome_options.add_argument(\"--enable-features=NetworkServiceInProcess\")\n",
    "\t\tchrome_options.add_argument(\"--disable-features=NetworkService\")\n",
    "\t\tchrome_options.add_argument(\n",
    "\t\t    \"user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/93.0.4577.63 Safari/537.36\"\n",
    "\t\t)\n",
    "\t\tdriver = webdriver.Chrome(options=chrome_options)\n",
    "\t\tdriver.maximize_window()\n",
    "\t\treturn driver\n",
    "\n",
    "\n",
    "\tdef _wait_for_page_to_load(self):\n",
    "\t\ttitle = self.driver.title\n",
    "\t\ttry:\n",
    "\t\t\tself.wait.until(\n",
    "\t\t\t\tlambda d: d.execute_script(\"return document.readyState\") == \"complete\"\n",
    "\t\t\t)\n",
    "\t\texcept:\n",
    "\t\t\tprint(f\"The webpage \\\"{title}\\\" did not get fully laoded.\\n\")\n",
    "\t\telse:\n",
    "\t\t\tprint(f\"The webpage \\\"{title}\\\" did get fully laoded.\\n\")\n",
    "\n",
    "\t\n",
    "\tdef access_website(self):\n",
    "\t\tself.driver.get(self.url)\n",
    "\t\tself._wait_for_page_to_load()\n",
    "\n",
    "\n",
    "\tdef search_properties(self, text):\n",
    "\t\t# locating and entering text in search bar\n",
    "\t\ttry:\n",
    "\t\t\tsearch_bar = self.wait.until(\n",
    "\t\t\t\tEC.presence_of_element_located((By.XPATH, '//*[@id=\"keyword2\"]'))\n",
    "\t\t\t)\n",
    "\t\texcept:\n",
    "\t\t\tprint(\"Timeout while locating Search Bar.\\n\")\n",
    "\t\telse:\n",
    "\t\t\tsearch_bar.send_keys(text)\n",
    "\t\t\ttime.sleep(2)\n",
    "\t\t\n",
    "\t\t# selecting valid option from list\n",
    "\t\ttry:\n",
    "\t\t\tvalid_option = self.wait.until(\n",
    "\t\t\t\tEC.element_to_be_clickable((By.XPATH, '//*[@id=\"0\"]'))\n",
    "\t\t\t)\n",
    "\t\texcept:\n",
    "\t\t\tprint(\"Timeout while locating valid search option.\\n\")\n",
    "\t\telse:\n",
    "\t\t\tvalid_option.click()\n",
    "\t\t\ttime.sleep(2)\n",
    "\t\t\n",
    "\t\t# click on Search button\n",
    "\t\ttry:\n",
    "\t\t\tsearch_button = self.wait.until(\n",
    "\t\t\t\tEC.element_to_be_clickable((By.XPATH, '//*[@id=\"searchform_search_btn\"]'))\n",
    "\t\t\t)\n",
    "\t\texcept:\n",
    "\t\t\tprint(\"Timeout while clicking on \\\"Search\\\" button.\\n\")\n",
    "\t\telse:\n",
    "\t\t\tsearch_button.click()\n",
    "\t\t\tself._wait_for_page_to_load()\n",
    "\n",
    "\n",
    "\tdef adjust_budget_slider(self, offset):\n",
    "\t\ttry:\n",
    "\t\t\tslider = self.wait.until(\n",
    "\t\t\t\tEC.element_to_be_clickable((By.XPATH, '//*[@id=\"budgetLeftFilter_max_node\"]'))\n",
    "\t\t\t)\n",
    "\t\texcept:\n",
    "\t\t\tprint(\"Timeout while clicking on Budget slider circle.\\n\")\n",
    "\t\telse:\n",
    "\t\t\tactions = ActionChains(self.driver)\n",
    "\t\t\t(\n",
    "\t\t\t\tactions\n",
    "\t\t\t\t.click_and_hold(slider)\n",
    "\t\t\t\t.move_by_offset(offset, 0)\n",
    "\t\t\t\t.release()\n",
    "\t\t\t\t.perform()\n",
    "\t\t\t)\n",
    "\t\t\ttime.sleep(2)\n",
    "\n",
    "\n",
    "\tdef apply_filters(self):\n",
    "\t\t# 1. Verified\n",
    "\t\tverified = self.wait.until(\n",
    "\t\t\tEC.element_to_be_clickable((By.XPATH, '/html[1]/body[1]/div[1]/div[1]/div[1]/div[4]/div[3]/div[1]/div[3]/section[1]/div[1]/div[1]/div[1]/div[1]/div[1]/div[1]/div[3]/span[2]'))\n",
    "\t\t)\n",
    "\t\tverified.click()\n",
    "\t\ttime.sleep(1)\n",
    "\t\t\n",
    "\t\t# 2. Ready To Move\n",
    "\t\tready_to_move = self.wait.until(\n",
    "\t\t\tEC.element_to_be_clickable((By.XPATH, '/html[1]/body[1]/div[1]/div[1]/div[1]/div[4]/div[3]/div[1]/div[3]/section[1]/div[1]/div[1]/div[1]/div[1]/div[1]/div[1]/div[5]/span[2]'))\n",
    "\t\t)\n",
    "\t\tready_to_move.click()\n",
    "\t\ttime.sleep(1)\n",
    "\t\t\n",
    "\t\t# moving to the right side to unhide remaining filters\n",
    "\t\twhile True:\n",
    "\t\t\ttry:\n",
    "\t\t\t\tfilter_right_button = self.wait.until(\n",
    "\t\t\t\t\tEC.presence_of_element_located((By.XPATH, \"//i[contains(@class,'iconS_Common_24 icon_upArrow cc__rightArrow')]\"))\n",
    "\t\t\t\t)\n",
    "\t\t\texcept:\n",
    "\t\t\t\tprint(\"Timeout because we have uncovered all filters.\\n\")\n",
    "\t\t\t\tbreak\n",
    "\t\t\telse:\n",
    "\t\t\t\tfilter_right_button.click()\n",
    "\t\t\t\ttime.sleep(1)\n",
    "\t\t\n",
    "\t\t# 3. With Photos\n",
    "\t\twith_photos = self.wait.until(\n",
    "\t\t\tEC.element_to_be_clickable((By.XPATH, '/html[1]/body[1]/div[1]/div[1]/div[1]/div[4]/div[3]/div[1]/div[3]/section[1]/div[1]/div[1]/div[1]/div[1]/div[2]/div[1]/div[6]/span[2]'))\n",
    "\t\t)\n",
    "\t\twith_photos.click()\n",
    "\t\ttime.sleep(1)\n",
    "\t\t\n",
    "\t\t# 4. With Videos\n",
    "\t\twith_videos = self.wait.until(\n",
    "\t\t\tEC.element_to_be_clickable((By.XPATH, '/html[1]/body[1]/div[1]/div[1]/div[1]/div[4]/div[3]/div[1]/div[3]/section[1]/div[1]/div[1]/div[1]/div[1]/div[2]/div[1]/div[7]/span[2]'))\n",
    "\t\t)\n",
    "\t\twith_videos.click()\n",
    "\t\ttime.sleep(3)\n",
    "\n",
    "\n",
    "\tdef _extract_data(self, row, by, value):\n",
    "\t\ttry:\n",
    "\t\t\treturn row.find_element(by, value).text\n",
    "\t\texcept:\n",
    "\t\t\treturn np.nan\n",
    "\t\n",
    "\n",
    "\tdef scrape_webpage(self):\n",
    "\t\trows = self.driver.find_elements(By.CLASS_NAME, \"tupleNew__TupleContent\")\n",
    "\t\tfor row in rows:\n",
    "\t\t\tproperty = {\n",
    "\t\t\t\t\"name\": self._extract_data(row, By.CLASS_NAME, \"tupleNew__headingNrera\"),\n",
    "\t\t\t\t\"location\": self._extract_data(row, By.CLASS_NAME, \"tupleNew__propType\"),\n",
    "\t\t\t\t\"price\": self._extract_data(row, By.CLASS_NAME, \"tupleNew__priceValWrap\")\n",
    "\t\t\t}\n",
    "\t\t\n",
    "\t\t\ttry:\n",
    "\t\t\t\telements = row.find_elements(By.CLASS_NAME, \"tupleNew__area1Type\")\n",
    "\t\t\texcept:\n",
    "\t\t\t\tproperty[\"area\"], property[\"bhk\"] = [np.nan, np.nan]\n",
    "\t\t\telse:\n",
    "\t\t\t\tproperty[\"area\"], property[\"bhk\"] = [ele.text for ele in elements]\n",
    "\t\t\t\t\n",
    "\t\t\tself.data.append(property)\n",
    "\t\t\n",
    "\n",
    "\tdef navigate_pages_and_scrape_data(self):\n",
    "\t\tpage_count = 0\n",
    "\t\twhile True:\n",
    "\t\t\tpage_count += 1\n",
    "\t\t\ttry:\n",
    "\t\t\t\tself.scrape_webpage()\n",
    "\t\t\t\tnext_page_button = self.driver.find_element(By.XPATH, \"//a[normalize-space()='Next Page >']\")\n",
    "\t\t\texcept:\n",
    "\t\t\t\tprint(f\"We have scraped {page_count} pages.\\n\")\n",
    "\t\t\t\tbreak\n",
    "\t\t\telse:\n",
    "\t\t\t\ttry:\n",
    "\t\t\t\t\tself.driver.execute_script(\"window.scrollBy(0, arguments[0].getBoundingClientRect().top - 100);\", next_page_button)\n",
    "\t\t\t\t\ttime.sleep(2)\t\t\t\n",
    "\t\t\t\t\tself.wait.until(\n",
    "\t\t\t\t\t\tEC.element_to_be_clickable((By.XPATH, \"//a[normalize-space()='Next Page >']\"))\n",
    "\t\t\t\t\t).click()\n",
    "\t\t\t\t\ttime.sleep(5)\n",
    "\t\t\t\texcept:\n",
    "\t\t\t\t\tprint(\"Timeout while clicking on \\\"Next Page\\\".\\n\")\n",
    "\n",
    "\n",
    "\tdef clean_data_and_save_as_excel(self, file_name):\n",
    "\t\tdf_properties = (\n",
    "\t\t\tpd\n",
    "\t\t\t.DataFrame(self.data)\n",
    "\t\t\t.drop_duplicates()\n",
    "\t\t\t.apply(lambda col: col.str.strip().str.lower() if col.dtype == \"object\" else col)\n",
    "\t\t\t.assign(\n",
    "\t\t\t\tis_starred=lambda df_: df_.name.str.contains(\"\\n\").astype(int),\n",
    "\t\t\t\tname=lambda df_: (\n",
    "\t\t\t\t\tdf_\n",
    "\t\t\t\t\t.name\n",
    "\t\t\t\t\t.str.replace(\"\\n[0-9.]+\", \"\", regex=True)\n",
    "\t\t\t\t\t.str.strip()\n",
    "\t\t\t\t\t.replace(\"adroit district s\", \"adroit district's\")\n",
    "\t\t\t\t),\n",
    "\t\t\t\tlocation=lambda df_: (\n",
    "\t\t\t\t\tdf_\n",
    "\t\t\t\t\t.location\n",
    "\t\t\t\t\t.str.replace(\"chennai\", \"\")\n",
    "\t\t\t\t\t.str.strip()\n",
    "\t\t\t\t\t.str.replace(\",$\", \"\", regex=True)\n",
    "\t\t\t\t\t.str.split(\"in\")\n",
    "\t\t\t\t\t.str[-1]\n",
    "\t\t\t\t\t.str.strip()\n",
    "\t\t\t\t),\n",
    "\t\t\t\tprice=lambda df_: (\n",
    "\t\t\t\t\tdf_\n",
    "\t\t\t\t\t.price\n",
    "\t\t\t\t\t.str.replace(\"₹\", \"\")\n",
    "\t\t\t\t\t.apply(lambda val: float(val.replace(\"lac\", \"\").strip()) if \"lac\" in val else float(val.replace(\"cr\", \"\").strip()) * 100)\n",
    "\t\t\t\t),\n",
    "\t\t\t\tarea=lambda df_: (\n",
    "\t\t\t\t\tdf_\n",
    "\t\t\t\t\t.area\n",
    "\t\t\t\t\t.str.replace(\"sqft\", \"\")\n",
    "\t\t\t\t\t.str.strip()\n",
    "\t\t\t\t\t.str.replace(\",\", \"\")\n",
    "\t\t\t\t\t.pipe(lambda ser: pd.to_numeric(ser))\n",
    "\t\t\t\t),\n",
    "\t\t\t\tbhk=lambda df_: (\n",
    "\t\t\t\t\tdf_\n",
    "\t\t\t\t\t.bhk\n",
    "\t\t\t\t\t.str.replace(\"bhk\", \"\")\n",
    "\t\t\t\t\t.str.strip()\n",
    "\t\t\t\t\t.pipe(lambda ser: pd.to_numeric(ser))\n",
    "\t\t\t\t)\n",
    "\t\t\t)\n",
    "\t\t\t.rename(columns={\n",
    "\t\t\t\t\"price\": \"price_lakhs\",\n",
    "\t\t\t\t\"area\": \"area_sqft\"\n",
    "\t\t\t})\n",
    "\t\t\t.reset_index(drop=True)\n",
    "\t\t)\n",
    "\t\tdf_properties.to_csv(\"thane-properties.csv\", index=False)\n",
    "\n",
    "\t\n",
    "\tdef run(self, text=\"Thane\", offset=-100, file_name=\"properties\"):\n",
    "\t\ttry:\n",
    "\t\t\tself.access_website()\n",
    "\t\t\tself.search_properties(text)\n",
    "\t\t\tself.adjust_budget_slider(offset)\n",
    "\t\t\tself.apply_filters()\n",
    "\t\t\tself.navigate_pages_and_scrape_data()\n",
    "\t\t\tself.clean_data_and_save_as_excel(file_name)\n",
    "\t\tfinally:\n",
    "\t\t\ttime.sleep(2)\n",
    "\t\t\tself.driver.quit()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\tscraper = ProprertyScraper(url=\"https://www.99acres.com/\")\n",
    "\tscraper.run(\n",
    "\t\ttext=\"thane\",\n",
    "\t\toffset=-73,\n",
    "\t\tfile_name=\"thaneproperties\"\n",
    "\t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d7bc2dc-db55-4b9b-909f-34e6a77ecafe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eaa73c4e-567c-42a8-adf5-396235f41339",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The webpage \"India Real Estate Property Site - Buy Sell Rent Properties Portal - 99acres.com\" did get fully laoded.\n",
      "\n",
      "The webpage \"Property in Thane - Real Estate in Thane\" did get fully laoded.\n",
      "\n",
      "Timeout because we have uncovered all filters.\n",
      "\n",
      "Timeout while clicking on \"Next Page\".\n",
      "\n",
      "Timeout because we have navigated all the 2 pages.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "# ----- SCRAPING THE DATA -----\n",
    "\n",
    "def wait_for_page_to_load(driver, wait):\n",
    "\ttitle = driver.title\n",
    "\ttry:\n",
    "\t\twait.until(\n",
    "\t\t\tlambda d: d.execute_script(\"return document.readyState\") == \"complete\"\n",
    "\t\t)\n",
    "\texcept:\n",
    "\t\tprint(f\"The webpage \\\"{title}\\\" did not get fully laoded.\\n\")\n",
    "\telse:\n",
    "\t\tprint(f\"The webpage \\\"{title}\\\" did get fully laoded.\\n\")\n",
    "\n",
    "\n",
    "# options\n",
    "chrome_options = Options()\n",
    "chrome_options.add_argument(\"--disable-http2\")\n",
    "chrome_options.add_argument(\"--incognito\")\n",
    "chrome_options.add_argument(\"--disable-blink-features=AutomationControlled\")\n",
    "chrome_options.add_argument(\"--ignore-certificate-errors\")\n",
    "chrome_options.add_argument(\"--enable-features=NetworkServiceInProcess\")\n",
    "chrome_options.add_argument(\"--disable-features=NetworkService\")\n",
    "chrome_options.add_argument(\n",
    "    \"user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/93.0.4577.63 Safari/537.36\"\n",
    ")\n",
    "\n",
    "driver = webdriver.Chrome(options=chrome_options)\n",
    "driver.maximize_window()\n",
    "\n",
    "# explicit wait\n",
    "wait = WebDriverWait(driver, 5)\n",
    "\n",
    "# accessing the target webpage\n",
    "url = \"https://www.99acres.com/\"\n",
    "driver.get(url)\n",
    "wait_for_page_to_load(driver, wait)\n",
    "\n",
    "# identify and enter text into search bar\n",
    "try:\n",
    "\tsearch_bar = wait.until(\n",
    "\t\tEC.presence_of_element_located((By.XPATH, '//*[@id=\"keyword2\"]'))\n",
    "\t)\n",
    "except:\n",
    "\tprint(\"Timeout while locating Search Bar.\\n\")\n",
    "else:\n",
    "\tsearch_bar.send_keys(\"Thane\")\n",
    "\ttime.sleep(2)\n",
    "\n",
    "# selecting valid option from list\n",
    "try:\n",
    "\tvalid_option = wait.until(\n",
    "\t\tEC.element_to_be_clickable((By.XPATH, '//*[@id=\"0\"]'))\n",
    "\t)\n",
    "except:\n",
    "\tprint(\"Timeout while locating valid search option.\\n\")\n",
    "else:\n",
    "\tvalid_option.click()\n",
    "\ttime.sleep(2)\n",
    "\n",
    "# click on Search button\n",
    "try:\n",
    "\tsearch_button = wait.until(\n",
    "\t\tEC.element_to_be_clickable((By.XPATH, '//*[@id=\"searchform_search_btn\"]'))\n",
    "\t)\n",
    "except:\n",
    "\tprint(\"Timeout while clicking on \\\"Search\\\" button.\\n\")\n",
    "else:\n",
    "\tsearch_button.click()\n",
    "\twait_for_page_to_load(driver, wait)\n",
    "\n",
    "# adjust the Budget slider\n",
    "try:\n",
    "\tslider = wait.until(\n",
    "\t\tEC.element_to_be_clickable((By.XPATH, '//*[@id=\"budgetLeftFilter_max_node\"]'))\n",
    "\t)\n",
    "except:\n",
    "\tprint(\"Timeout while clicking on Budget slider circle.\\n\")\n",
    "else:\n",
    "\tactions = ActionChains(driver)\n",
    "\t(\n",
    "\t\tactions\n",
    "\t\t.click_and_hold(slider)\n",
    "\t\t.move_by_offset(-73, 0)\n",
    "\t\t.release()\n",
    "\t\t.perform()\n",
    "\t)\n",
    "\ttime.sleep(2)\n",
    "\n",
    "# filter results to show genuine listings\n",
    "# 1. Verified\n",
    "verified = wait.until(\n",
    "\tEC.element_to_be_clickable((By.XPATH, '/html[1]/body[1]/div[1]/div[1]/div[1]/div[4]/div[3]/div[1]/div[3]/section[1]/div[1]/div[1]/div[1]/div[1]/div[1]/div[1]/div[3]/span[2]'))\n",
    ")\n",
    "verified.click()\n",
    "time.sleep(1)\n",
    "\n",
    "# 2. Ready To Move\n",
    "ready_to_move = wait.until(\n",
    "\tEC.element_to_be_clickable((By.XPATH, '/html[1]/body[1]/div[1]/div[1]/div[1]/div[4]/div[3]/div[1]/div[3]/section[1]/div[1]/div[1]/div[1]/div[1]/div[1]/div[1]/div[5]/span[2]'))\n",
    ")\n",
    "ready_to_move.click()\n",
    "time.sleep(1)\n",
    "\n",
    "# moving to the right side to unhide remaining filters\n",
    "while True:\n",
    "\ttry:\n",
    "\t\tfilter_right_button = wait.until(\n",
    "\t\t\tEC.presence_of_element_located((By.XPATH, \"//i[contains(@class,'iconS_Common_24 icon_upArrow cc__rightArrow')]\"))\n",
    "\t\t)\n",
    "\texcept:\n",
    "\t\tprint(\"Timeout because we have uncovered all filters.\\n\")\n",
    "\t\tbreak\n",
    "\telse:\n",
    "\t\tfilter_right_button.click()\n",
    "\t\ttime.sleep(1)\n",
    "\n",
    "# 3. With Photos\n",
    "with_photos = wait.until(\n",
    "\tEC.element_to_be_clickable((By.XPATH, '/html[1]/body[1]/div[1]/div[1]/div[1]/div[4]/div[3]/div[1]/div[3]/section[1]/div[1]/div[1]/div[1]/div[1]/div[2]/div[1]/div[6]/span[2]'))\n",
    ")\n",
    "with_photos.click()\n",
    "time.sleep(1)\n",
    "\n",
    "# 4. With Videos\n",
    "with_videos = wait.until(\n",
    "\tEC.element_to_be_clickable((By.XPATH, '/html[1]/body[1]/div[1]/div[1]/div[1]/div[4]/div[3]/div[1]/div[3]/section[1]/div[1]/div[1]/div[1]/div[1]/div[2]/div[1]/div[7]/span[2]'))\n",
    ")\n",
    "with_videos.click()\n",
    "time.sleep(3)\n",
    "\n",
    "# navigate pages and extract data\n",
    "data = []\n",
    "page_count = 0\n",
    "while True:\n",
    "\tpage_count += 1\n",
    "\ttry:\n",
    "\t\tnext_page_button = driver.find_element(By.XPATH, \"//a[normalize-space()='Next Page >']\")\n",
    "\texcept:\n",
    "\t\tprint(f\"Timeout because we have navigated all the {page_count} pages.\\n\")\n",
    "\t\tbreak\n",
    "\telse:\n",
    "\t\ttry:\n",
    "\t\t\tdriver.execute_script(\"window.scrollBy(0, arguments[0].getBoundingClientRect().top - 100);\", next_page_button)\n",
    "\t\t\ttime.sleep(2)\n",
    "\t\n",
    "\t\t\t# scraping the data\n",
    "\t\t\trows = driver.find_elements(By.CLASS_NAME, \"tupleNew__TupleContent\")\n",
    "\t\t\tfor row in rows:\n",
    "\t\t\t\t# property name\n",
    "\t\t\t\ttry:\n",
    "\t\t\t\t\tname = row.find_element(By.CLASS_NAME, \"tupleNew__headingNrera\").text\n",
    "\t\t\t\texcept:\n",
    "\t\t\t\t\tname = np.nan\n",
    "\n",
    "\t\t\t\t# property location\n",
    "\t\t\t\ttry:\n",
    "\t\t\t\t\tlocation = row.find_element(By.CLASS_NAME, \"tupleNew__propType\").text\n",
    "\t\t\t\texcept:\n",
    "\t\t\t\t\tlocation = np.nan\n",
    "\n",
    "\t\t\t\t# property price\n",
    "\t\t\t\ttry:\n",
    "\t\t\t\t\tprice = row.find_element(By.CLASS_NAME, \"tupleNew__priceValWrap\").text\n",
    "\t\t\t\texcept:\n",
    "\t\t\t\t\tprice = np.nan\n",
    "\n",
    "\t\t\t\t# property area and bhk\n",
    "\t\t\t\ttry:\n",
    "\t\t\t\t\telements = row.find_elements(By.CLASS_NAME, \"tupleNew__area1Type\")\n",
    "\t\t\t\texcept:\n",
    "\t\t\t\t\tarea, bhk = [np.nan, np.nan]\n",
    "\t\t\t\telse:\n",
    "\t\t\t\t\tarea, bhk = [ele.text for ele in elements]\n",
    "\t\t\t\t\t\n",
    "\t\t\t\tproperty = {\n",
    "\t\t\t\t\t\"name\": name,\n",
    "\t\t\t\t\t\"location\": location,\n",
    "\t\t\t\t\t\"price\": price,\n",
    "\t\t\t\t\t\"area\": area,\n",
    "\t\t\t\t\t\"bhk\": bhk\n",
    "\t\t\t\t}\n",
    "\t\t\t\tdata.append(property)\n",
    "\t\t\t\n",
    "\t\t\twait.until(\n",
    "\t\t\t\tEC.element_to_be_clickable((By.XPATH, \"//a[normalize-space()='Next Page >']\"))\n",
    "\t\t\t).click()\n",
    "\t\t\ttime.sleep(5)\n",
    "\t\texcept:\n",
    "\t\t\tprint(\"Timeout while clicking on \\\"Next Page\\\".\\n\")\n",
    "\n",
    "# scraping data from the last page\n",
    "rows = driver.find_elements(By.CLASS_NAME, \"tupleNew__TupleContent\")\n",
    "for row in rows:\n",
    "\t# property name\n",
    "\ttry:\n",
    "\t\tname = row.find_element(By.CLASS_NAME, \"tupleNew__headingNrera\").text\n",
    "\texcept:\n",
    "\t\tname = np.nan\n",
    "\n",
    "\t# property location\n",
    "\ttry:\n",
    "\t\tlocation = row.find_element(By.CLASS_NAME, \"tupleNew__propType\").text\n",
    "\texcept:\n",
    "\t\tlocation = np.nan\n",
    "\n",
    "\t# property price\n",
    "\ttry:\n",
    "\t\tprice = row.find_element(By.CLASS_NAME, \"tupleNew__priceValWrap\").text\n",
    "\texcept:\n",
    "\t\tprice = np.nan\n",
    "\n",
    "\t# property area and bhk\n",
    "\ttry:\n",
    "\t\telements = row.find_elements(By.CLASS_NAME, \"tupleNew__area1Type\")\n",
    "\texcept:\n",
    "\t\tarea, bhk = [np.nan, np.nan]\n",
    "\telse:\n",
    "\t\tarea, bhk = [ele.text for ele in elements]\n",
    "\t\t\t\t\t\n",
    "\tproperty = {\n",
    "\t\t\"name\": name,\n",
    "\t\t\"location\": location,\n",
    "\t\t\"price\": price,\n",
    "\t\t\"area\": area,\n",
    "\t\t\"bhk\": bhk\n",
    "\t}\n",
    "\tdata.append(property)\n",
    "\n",
    "time.sleep(2)\n",
    "driver.quit()\n",
    "\n",
    "# ----- CLEANING THE DATA -----\n",
    "\n",
    "df_properties = (\n",
    "\tpd\n",
    "\t.DataFrame(data)\n",
    "\t.drop_duplicates()\n",
    "\t.apply(lambda col: col.str.strip().str.lower() if col.dtype == \"object\" else col)\n",
    "\t.assign(\n",
    "\t\tis_starred=lambda df_: df_.name.str.contains(\"\\n\").astype(int),\n",
    "\t\tname=lambda df_: (\n",
    "\t\t\tdf_\n",
    "\t\t\t.name\n",
    "\t\t\t.str.replace(\"\\n[0-9.]+\", \"\", regex=True)\n",
    "\t\t\t.str.strip()\n",
    "\t\t\t.replace(\"adroit district s\", \"adroit district's\")\n",
    "\t\t),\n",
    "\t\tlocation=lambda df_: (\n",
    "\t\t\tdf_\n",
    "\t\t\t.location\n",
    "\t\t\t.str.replace(\"thane\", \"\")\n",
    "\t\t\t.str.strip()\n",
    "\t\t\t.str.replace(\",$\", \"\", regex=True)\n",
    "\t\t\t.str.split(\"in\")\n",
    "\t\t\t.str[-1]\n",
    "\t\t\t.str.strip()\n",
    "\t\t),\n",
    "\t\tprice=lambda df_: (\n",
    "\t\t\tdf_\n",
    "\t\t\t.price\n",
    "\t\t\t.str.replace(\"₹\", \"\")\n",
    "\t\t\t.apply(lambda val: float(val.replace(\"lac\", \"\").strip()) if \"lac\" in val else float(val.replace(\"cr\", \"\").strip()) * 100)\n",
    "\t\t),\n",
    "\t\tarea=lambda df_: (\n",
    "\t\t\tdf_\n",
    "\t\t\t.area\n",
    "\t\t\t.str.replace(\"sqft\", \"\")\n",
    "\t\t\t.str.strip()\n",
    "\t\t\t.str.replace(\",\", \"\")\n",
    "\t\t\t.pipe(lambda ser: pd.to_numeric(ser))\n",
    "\t\t),\n",
    "\t\tbhk=lambda df_: (\n",
    "\t\t\tdf_\n",
    "\t\t\t.bhk\n",
    "\t\t\t.str.replace(\"bhk\", \"\")\n",
    "\t\t\t.str.strip()\n",
    "\t\t\t.pipe(lambda ser: pd.to_numeric(ser))\n",
    "\t\t)\n",
    "\t)\n",
    "\t.rename(columns={\n",
    "\t\t\"price\": \"price_lakhs\",\n",
    "\t\t\"area\": \"area_sqft\"\n",
    "\t})\n",
    "\t.reset_index(drop=True)\n",
    "\t.to_csv(\"thane-properties.csv\", index=False)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6f8a76c-6996-467f-b8bb-f6dfcdf5ff3a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
